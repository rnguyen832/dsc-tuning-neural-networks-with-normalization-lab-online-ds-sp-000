{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Neural Networks with Normalization - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For this lab on initialization and optimization, you'll build a neural network to perform a regression task.\n",
    "\n",
    "It is worth noting that getting regression to work with neural networks can be difficult because the output is unbounded ($\\hat y$ can technically range from $-\\infty$ to $+\\infty$, and the models are especially prone to exploding gradients. This issue makes a regression exercise the perfect learning case for tinkering with normalization and optimization strategies to ensure proper convergence!\n",
    "\n",
    "## Objectives\n",
    "You will be able to:\n",
    "* Build a neural network using Keras\n",
    "* Normalize your data to assist algorithm convergence\n",
    "* Implement and observe the impact of various initialization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras import initializers\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we'll be working with is data related to Facebook posts published during the year of 2014 on the Facebook page of a renowned cosmetics brand.  It includes 7 features known prior to post publication, and 12 features for evaluating the post impact. What we want to do is make a predictor for the number of \"likes\" for a post, taking into account the 7 features prior to posting.\n",
    "\n",
    "First, let's import the data set, `dataset_Facebook.csv`, and delete any rows with missing data. Afterwards, briefly preview the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 19)\n",
      "(495, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page total likes</th>\n",
       "      <th>Type</th>\n",
       "      <th>Category</th>\n",
       "      <th>Post Month</th>\n",
       "      <th>Post Weekday</th>\n",
       "      <th>Post Hour</th>\n",
       "      <th>Paid</th>\n",
       "      <th>Lifetime Post Total Reach</th>\n",
       "      <th>Lifetime Post Total Impressions</th>\n",
       "      <th>Lifetime Engaged Users</th>\n",
       "      <th>Lifetime Post Consumers</th>\n",
       "      <th>Lifetime Post Consumptions</th>\n",
       "      <th>Lifetime Post Impressions by people who have liked your Page</th>\n",
       "      <th>Lifetime Post reach by people who like your Page</th>\n",
       "      <th>Lifetime People who have liked your Page and engaged with your post</th>\n",
       "      <th>comment</th>\n",
       "      <th>like</th>\n",
       "      <th>share</th>\n",
       "      <th>Total Interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2752</td>\n",
       "      <td>5091</td>\n",
       "      <td>178</td>\n",
       "      <td>109</td>\n",
       "      <td>159</td>\n",
       "      <td>3078</td>\n",
       "      <td>1640</td>\n",
       "      <td>119</td>\n",
       "      <td>4</td>\n",
       "      <td>79.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>139441</td>\n",
       "      <td>Status</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10460</td>\n",
       "      <td>19057</td>\n",
       "      <td>1457</td>\n",
       "      <td>1361</td>\n",
       "      <td>1674</td>\n",
       "      <td>11710</td>\n",
       "      <td>6112</td>\n",
       "      <td>1108</td>\n",
       "      <td>5</td>\n",
       "      <td>130.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2413</td>\n",
       "      <td>4373</td>\n",
       "      <td>177</td>\n",
       "      <td>113</td>\n",
       "      <td>154</td>\n",
       "      <td>2812</td>\n",
       "      <td>1503</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50128</td>\n",
       "      <td>87991</td>\n",
       "      <td>2211</td>\n",
       "      <td>790</td>\n",
       "      <td>1119</td>\n",
       "      <td>61027</td>\n",
       "      <td>32048</td>\n",
       "      <td>1386</td>\n",
       "      <td>58</td>\n",
       "      <td>1572.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>1777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>139441</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7244</td>\n",
       "      <td>13594</td>\n",
       "      <td>671</td>\n",
       "      <td>410</td>\n",
       "      <td>580</td>\n",
       "      <td>6228</td>\n",
       "      <td>3200</td>\n",
       "      <td>396</td>\n",
       "      <td>19</td>\n",
       "      <td>325.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>494</td>\n",
       "      <td>85093</td>\n",
       "      <td>Photo</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5400</td>\n",
       "      <td>9218</td>\n",
       "      <td>810</td>\n",
       "      <td>756</td>\n",
       "      <td>1003</td>\n",
       "      <td>5654</td>\n",
       "      <td>3230</td>\n",
       "      <td>422</td>\n",
       "      <td>10</td>\n",
       "      <td>125.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>85093</td>\n",
       "      <td>Photo</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4684</td>\n",
       "      <td>7536</td>\n",
       "      <td>733</td>\n",
       "      <td>708</td>\n",
       "      <td>985</td>\n",
       "      <td>4750</td>\n",
       "      <td>2876</td>\n",
       "      <td>392</td>\n",
       "      <td>5</td>\n",
       "      <td>53.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>81370</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3480</td>\n",
       "      <td>6229</td>\n",
       "      <td>537</td>\n",
       "      <td>508</td>\n",
       "      <td>687</td>\n",
       "      <td>3961</td>\n",
       "      <td>2104</td>\n",
       "      <td>301</td>\n",
       "      <td>0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>81370</td>\n",
       "      <td>Photo</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3778</td>\n",
       "      <td>7216</td>\n",
       "      <td>625</td>\n",
       "      <td>572</td>\n",
       "      <td>795</td>\n",
       "      <td>4742</td>\n",
       "      <td>2388</td>\n",
       "      <td>363</td>\n",
       "      <td>4</td>\n",
       "      <td>93.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>81370</td>\n",
       "      <td>Photo</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4156</td>\n",
       "      <td>7564</td>\n",
       "      <td>626</td>\n",
       "      <td>574</td>\n",
       "      <td>832</td>\n",
       "      <td>4534</td>\n",
       "      <td>2452</td>\n",
       "      <td>370</td>\n",
       "      <td>7</td>\n",
       "      <td>91.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>495 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Page total likes    Type  Category  Post Month  Post Weekday  Post Hour  \\\n",
       "0              139441   Photo         2          12             4          3   \n",
       "1              139441  Status         2          12             3         10   \n",
       "2              139441   Photo         3          12             3          3   \n",
       "3              139441   Photo         2          12             2         10   \n",
       "4              139441   Photo         2          12             2          3   \n",
       "..                ...     ...       ...         ...           ...        ...   \n",
       "494             85093   Photo         3           1             7         10   \n",
       "495             85093   Photo         3           1             7          2   \n",
       "496             81370   Photo         2           1             5          8   \n",
       "497             81370   Photo         1           1             5          2   \n",
       "498             81370   Photo         3           1             4         11   \n",
       "\n",
       "     Paid  Lifetime Post Total Reach  Lifetime Post Total Impressions  \\\n",
       "0     0.0                       2752                             5091   \n",
       "1     0.0                      10460                            19057   \n",
       "2     0.0                       2413                             4373   \n",
       "3     1.0                      50128                            87991   \n",
       "4     0.0                       7244                            13594   \n",
       "..    ...                        ...                              ...   \n",
       "494   0.0                       5400                             9218   \n",
       "495   0.0                       4684                             7536   \n",
       "496   0.0                       3480                             6229   \n",
       "497   0.0                       3778                             7216   \n",
       "498   0.0                       4156                             7564   \n",
       "\n",
       "     Lifetime Engaged Users  Lifetime Post Consumers  \\\n",
       "0                       178                      109   \n",
       "1                      1457                     1361   \n",
       "2                       177                      113   \n",
       "3                      2211                      790   \n",
       "4                       671                      410   \n",
       "..                      ...                      ...   \n",
       "494                     810                      756   \n",
       "495                     733                      708   \n",
       "496                     537                      508   \n",
       "497                     625                      572   \n",
       "498                     626                      574   \n",
       "\n",
       "     Lifetime Post Consumptions  \\\n",
       "0                           159   \n",
       "1                          1674   \n",
       "2                           154   \n",
       "3                          1119   \n",
       "4                           580   \n",
       "..                          ...   \n",
       "494                        1003   \n",
       "495                         985   \n",
       "496                         687   \n",
       "497                         795   \n",
       "498                         832   \n",
       "\n",
       "     Lifetime Post Impressions by people who have liked your Page  \\\n",
       "0                                                 3078              \n",
       "1                                                11710              \n",
       "2                                                 2812              \n",
       "3                                                61027              \n",
       "4                                                 6228              \n",
       "..                                                 ...              \n",
       "494                                               5654              \n",
       "495                                               4750              \n",
       "496                                               3961              \n",
       "497                                               4742              \n",
       "498                                               4534              \n",
       "\n",
       "     Lifetime Post reach by people who like your Page  \\\n",
       "0                                                1640   \n",
       "1                                                6112   \n",
       "2                                                1503   \n",
       "3                                               32048   \n",
       "4                                                3200   \n",
       "..                                                ...   \n",
       "494                                              3230   \n",
       "495                                              2876   \n",
       "496                                              2104   \n",
       "497                                              2388   \n",
       "498                                              2452   \n",
       "\n",
       "     Lifetime People who have liked your Page and engaged with your post  \\\n",
       "0                                                  119                     \n",
       "1                                                 1108                     \n",
       "2                                                  132                     \n",
       "3                                                 1386                     \n",
       "4                                                  396                     \n",
       "..                                                 ...                     \n",
       "494                                                422                     \n",
       "495                                                392                     \n",
       "496                                                301                     \n",
       "497                                                363                     \n",
       "498                                                370                     \n",
       "\n",
       "     comment    like  share  Total Interactions  \n",
       "0          4    79.0   17.0                 100  \n",
       "1          5   130.0   29.0                 164  \n",
       "2          0    66.0   14.0                  80  \n",
       "3         58  1572.0  147.0                1777  \n",
       "4         19   325.0   49.0                 393  \n",
       "..       ...     ...    ...                 ...  \n",
       "494       10   125.0   41.0                 176  \n",
       "495        5    53.0   26.0                  84  \n",
       "496        0    53.0   22.0                  75  \n",
       "497        4    93.0   18.0                 115  \n",
       "498        7    91.0   38.0                 136  \n",
       "\n",
       "[495 rows x 19 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load the dataset and drop rows with missing values. Then preview the data.\n",
    "raw = pd.read_csv(\"dataset_Facebook.csv\", sep=\";\")\n",
    "df = raw.dropna()\n",
    "print(raw.shape)\n",
    "print(df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Problem\n",
    "\n",
    "Define X and Y and perform a train-validation-test split.\n",
    "\n",
    "X will be:\n",
    "* Page total likes\n",
    "* Post Month\n",
    "* Post Weekday\n",
    "* Post Hour\n",
    "* Paid\n",
    "along with dummy variables for:\n",
    "* Type\n",
    "* Category\n",
    "\n",
    "Y will be the `like` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; define the problem.\n",
    "X = pd.DataFrame(df, columns=[df.columns[i] for i in range(7)], index=range(len(df)))\n",
    "dummy_type = pd.get_dummies(X[\"Type\"], drop_first=True)\n",
    "dummy_category = pd.get_dummies(X[\"Category\"], drop_first=True)\n",
    "X = pd.concat([X, dummy_type, dummy_category], axis=1).drop(['Type', 'Category'], axis=1)\n",
    "\n",
    "Y = df[\"like\"]\n",
    "\n",
    "np.random.seed(123)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=123)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Baseline Model\n",
    "\n",
    "Next, build a naive baseline model to compare performance against is a helpful reference point. From there, you can then observe the impact of various tunning procedures which will iteratively improve your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simply run this code block, later you'll modify this model to tune the performance\n",
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"sgd\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Baseline\n",
    "\n",
    "Evaluate the baseline model for the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: nan\n",
      "Validation MSE: nan\n"
     ]
    }
   ],
   "source": [
    "#Your code here; evaluate the model with MSE\n",
    "train_predict = model.predict(X_train).reshape(-1)\n",
    "val_predict = model.predict(X_val).reshape(-1)\n",
    "\n",
    "train_MSE = np.mean((train_predict-Y_train)**2)\n",
    "val_MSE = np.mean((val_predict-Y_val)**2)\n",
    "\n",
    "print(\"Training MSE:\", train_MSE)\n",
    "print(\"Validation MSE:\", val_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan, nan, nan, nan, nan]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; inspect the loss function through the history object\n",
    "hist.history[\"loss\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice this extremely problematic behavior: all the values for training and validation loss are \"nan\". This indicates that the algorithm did not converge. The first solution to this is to normalize the input. From there, if convergence is not achieved, normalizing the output may also be required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the input features by subtracting each feature mean and dividing by the standard deviation in order to transform each into a standard normal distribution. Then recreate the train-validate-test sets with the transformed input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## standardize/categorize\n",
    "X = pd.DataFrame(df, columns=[df.columns[i] for i in range(7)])\n",
    "X.drop(['Type', 'Category'], axis=1, inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "X = pd.concat([X, dummy_type, dummy_category], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refit the Model and Reevaluate\n",
    "\n",
    "Great! Now refit the model and once again assess it's performance on the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; refit a model as shown above\n",
    "np.random.seed(123)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=123)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=123)\n",
    "\n",
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"sgd\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: nan\n",
      "Validation MSE: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[nan, nan, nan, nan, nan]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rexamine the loss function\n",
    "train_predict = model.predict(X_train).reshape(-1)\n",
    "val_predict = model.predict(X_val).reshape(-1)\n",
    "\n",
    "train_MSE = np.mean((train_predict-Y_train)**2)\n",
    "val_MSE = np.mean((val_predict-Y_val)**2)\n",
    "\n",
    "print(\"Training MSE:\", train_MSE)\n",
    "print(\"Validation MSE:\", val_MSE)\n",
    "\n",
    "hist.history[\"loss\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that you still haven't achieved convergence! From here, it's time to normalize the output data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the output\n",
    "\n",
    "Normalize Y as you did X by subtracting the mean and dividing by the standard deviation. Then, resplit the data into training and validation sets as we demonstrated above, and retrain a new model using your normalized X and Y data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here: redefine Y after normalizing the data.\n",
    "Y = (Y-np.mean(Y)) / np.std(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; create training and validation sets as before. Use random seed 123.\n",
    "np.random.seed(123)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=123)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; rebuild a simple model using a relu layer followed by a linear layer. (See our code snippet above!)\n",
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"sgd\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, reevaluate the updated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 1.0660808704938327\n",
      "Validation MSE: 0.9549355358499283\n"
     ]
    }
   ],
   "source": [
    "#Your code here; MSE\n",
    "train_predict = model.predict(X_train).reshape(-1)\n",
    "val_predict = model.predict(X_val).reshape(-1)\n",
    "\n",
    "train_MSE = np.mean((train_predict-Y_train)**2)\n",
    "val_MSE = np.mean((val_predict-Y_val)**2)\n",
    "\n",
    "print(\"Training MSE:\", train_MSE)\n",
    "print(\"Validation MSE:\", val_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5679563841458117,\n",
       " 1.4011991904692704,\n",
       " 1.3161553094226324,\n",
       " 1.2673761556657512,\n",
       " 1.231529721718156]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; loss function\n",
    "hist.history[\"loss\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that you have a converged model, you can also experiment with alternative optimizers and initialization strategies to see if you can find a better global minimum. (After all, the current models may have converged to a local minimum.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Weight Initializers\n",
    "\n",
    "Below, take a look at the code provided to see how to modify the neural network to use alternative initialization and optimization strategies. At the end, you'll then be asked to select the model which you believe is the strongest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  He Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=10, kernel_initializer= \"he_normal\",\n",
    "                activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"sgd\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val),verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0850157645954366\n",
      "1.0414259862943835\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train)\n",
    "print(MSE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecun Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=10, \n",
    "                kernel_initializer= \"lecun_normal\", activation='tanh'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"sgd\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0802171323618437\n",
      "0.9253934845476914\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train)\n",
    "print(MSE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much of a difference, but a useful note to consider when tuning your network. Next, let's investigate the impact of various optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"rmsprop\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0459603797200847\n",
      "1.0022574441873215\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train)\n",
    "print(MSE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"Adam\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0401462009035234\n",
      "1.0585266760166965\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train)\n",
    "print(MSE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Decay with Momentum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "sgd = optimizers.SGD(lr=0.03, decay=0.0001, momentum=0.9)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= sgd ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9227508966259036\n",
      "1.022192110404578\n"
     ]
    }
   ],
   "source": [
    "print(MSE_train)\n",
    "print(MSE_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a Final Model\n",
    "\n",
    "Now, select the model with the best performance based on the training and validation sets. Evaluate this top model using the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0689122473367239\n",
      "0.949402261853473 \n",
      "\n",
      "0.19939251636047\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "np.random.seed(123)\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(8, input_dim=10, kernel_initializer=\"lecun_normal\", activation='tanh'))\n",
    "model.add(layers.Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(optimizer= \"rmsprop\" ,loss='mse',metrics=['mse'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, \n",
    "                 epochs=100, validation_data = (X_val, Y_val), verbose = 0)\n",
    "\n",
    "pred_train = model.predict(X_train).reshape(-1)\n",
    "pred_val = model.predict(X_val).reshape(-1)\n",
    "\n",
    "MSE_train = np.mean((pred_train-Y_train)**2)\n",
    "MSE_val = np.mean((pred_val-Y_val)**2)\n",
    "\n",
    "test_predict = model.predict(X_test).reshape(-1)\n",
    "test_MSE = np.mean((test_predict-Y_test)**2)\n",
    "\n",
    "print(MSE_train)\n",
    "print(MSE_val, \"\\n\")\n",
    "print(test_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lab, you worked to ensure your model converged properly. Additionally, you also investigated the impact of varying initialization and optimization routines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
